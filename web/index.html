<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>顔認識</title>
  <style>
    /* video 要素の上に canvas 要素をオーバーレイするための CSS */
    #container { /* コンテナ用の div について */
      position: relative; /* 座標指定を相対値指定にする */
    }

    #video { /* カメラ映像を流す video について */
      transform: scaleX(-1); /* 左右反転させる */
    }

    #canvas { /* 描画用の canvas について */
      transform: scaleX(-1); /* 左右反転させる */
      position: absolute; /* 座標指定を絶対値指定にして */
      left: 0; /* X座標を0に */
      top: 0; /* Y座標を0に */
    }
  </style>
</head>
<body>
<div id="container">
  <!-- video の上に canvas をオーバーレイするため -->
  <video id="video" width="400" height="300" autoplay>
    <!-- カメラ映像を流す -->
  </video>
  <canvas id="canvas" width="400" height="300">
    <!-- 重ねて描画する -->
  </canvas>
</div>
<div id="dat">
  <!-- データ表示用 -->
</div>

<script src="./js/clmtrackr.min.js"></script>
<script src="./js/model/model_pca_20_svm.js"></script>

<script>
  var video = document.getElementById("video");
  var canvas = document.getElementById("canvas");
  var context = canvas.getContext("2d");
  var stampMouth = new Image();
  stampMouth.src = "./images/voe/voe_mouth.png";
  var stampNose = new Image();
  stampNose.src = "./images/voe/voe_nose.png";
  var stampRightEye = new Image();
  stampRightEye.src = "./images/voe/voe_eye.png";
  var stampLeftEye = new Image();
  stampLeftEye.src = "./images/voe/voe_eye.png";
  var stampRightEar = new Image();
  stampRightEar.src = "./images/voe/voe_left_ear.png";
  var stampLeftEar = new Image();
  stampLeftEar.src = "./images/voe/voe_right_ear.png";

  // getUserMedia によるカメラ映像の取得
  var media = navigator.mediaDevices.getUserMedia({
    video: {facingMode: "user"},
    audio: false
  });
  media.then((stream) => {
    video.src = window.URL.createObjectURL(stream);
  });

  // clmtrackr の開始
  var tracker = new clm.tracker();
  // tracker を所定のフェイスモデル（※）で初期化
  tracker.init(pModel);
  // video 要素内でフェイストラッキング開始
  tracker.start(video);

  // 描画ループ
  function drawLoop() {
    // drawLoop 関数を繰り返し実行
    requestAnimationFrame(drawLoop);
    // 顔部品の現在位置の取得
    var positions = tracker.getCurrentPosition();
    // canvas をクリア
    context.clearRect(0, 0, canvas.width, canvas.height);
    drawStamp(positions, stampMouth, 53, 1.5, 0.0, 0.0);
    drawStamp(positions, stampNose, 62, 0.5, 0.0, 0.0);
    drawStamp(positions, stampRightEye, 27, 0.5, 0.0, 0.0);
    drawStamp(positions, stampLeftEye, 32, 0.5, 0.0, 0.0);
    drawStamp(positions, stampRightEar, 13, 0.5, -0.25, 0.0);
    drawStamp(positions, stampLeftEar, 1, 0.5, -0.25, 0.0);
  }

  drawLoop();                                             // drawLoop 関数をトリガー

  // ★スタンプを描く drawStamp 関数
  // (顔部品の位置データ, 画像, 基準位置, 大きさ, 横シフト, 縦シフト)
  // 基準位置の参考 => https://github.com/auduno/clmtrackr
  function drawStamp(pos, img, bNo, scale, hShift, vShift) {
    // 幅の基準として両眼の間隔を求める
    var eyes = pos[32][0] - pos[27][0];
    // 高さの基準として眉間と鼻先の間隔を求める
    var nose = pos[62][1] - pos[33][1];
    // 両眼の間隔をもとに画像のスケールを決める
    var wScale = eyes / img.width;
    // 画像の幅をスケーリング
    var imgW = img.width * scale * wScale;
    // 画像の高さをスケーリング
    var imgH = img.height * scale * wScale;
    // 画像のLeftを決める
    var imgL = pos[bNo][0] - imgW / 2 + eyes * hShift;
    // 画像のTopを決める
    var imgT = pos[bNo][1] - imgH / 2 + nose * vShift;
    // 画像を描く
    context.drawImage(img, imgL, imgT, imgW, imgH);
  }
</script>
</body>
</html>
